---
title: "Elasticsearch-安装中文分词器"
date: "2021-08-16 16:00"
permalink: "2021-08-16-elasticsearch-ik"
---

# Elasticsearch 中文分词器

Elasticsearch 自带了一堆的分词器，比如`Standard Analyzer`、`Simple Analyzer`、`whitespace Analyzer`等分词器，但是都对中文分词的效果不太好，此处安装第三方分词器`ik`，来实现分词

## Elasticsearch 内置分词器

- **Standard Analyzer** - 默认分词器，按词切分，小写处理
- **Simple Analyzer** - 按照非字母切分(符号被过滤), 小写处理
- **Whitespace Analyzer** - 按照空格切分，不转小写
- Stop Analyzer - 小写处理，停用词过滤(the,a,is)
- Keyword Analyzer - 不分词，直接将输入当作输出
- Patter Analyzer - 正则表达式，默认\W+(非字符分割)
- Language - 提供了30多种常见语言的分词器
- Customer Analyzer 自定义分词器

## 安装 ik 分词器

开源分词器 `Ik` 的github：[https://github.com/medcl/elasticsearch-analysis-ik](https://github.com/medcl/elasticsearch-analysis-ik)

> **注意：** Ik 分词器的版本要和你安装的 Elasticsearch 版本一致，我使用的版本是 *7.14.0*

在 github 上找到对应的版本，使用 Elasticsearch 自带的插件管理 **`elasticsearch-plugin`** 来进行安装

- 直接从网络地址安装

  ```bash
  ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.14.0/elasticsearch-analysis-ik-7.14.0.zip
  # 查看是否安装完成
  ./bin/elasticsearch-plugin list
  ```

- 从本地安装

  ```bash
  # 下载插件(file后面跟的是插件在本地的地址)
  ./bin/elasticsearch-plugin install file:///xxxx/elasticsearch-analysis-ik-7.14.0.zip
  ```

> 注意：如果本地插件的路径中存在空格，需要使用双引号包装起来

安装完插件后需重启Es，才能生效

## IK 分词使用

IK 有两种颗粒度的拆分：

- **`ik_smart`** : 会做最粗粒度的拆分

- **`ik_max_word`** : 会将文本做最细粒度的拆分

### 默认分词拆分

```sh
GET _analyze
{
  "analyzer": "default",
  "text": "我是中国人"
}
```

执行结果如下👇

![QKrpA8](https://media.zenghr.cn/blog/img/20210818/QKrpA8.png)

### ik_smart 分词

```sh
GET _analyze
{
  "analyzer": "ik_smart",
  "text": "我是中国人"
}
```

执行结果如下👇

```json
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "CN_CHAR",
      "position" : 0
    },
    {
      "token" : "是",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "CN_CHAR",
      "position" : 1
    },
    {
      "token" : "中国人",
      "start_offset" : 2,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 2
    }
  ]
}
```

### ik_max_word 分词

```sh
GET _analyze
{
  "analyzer": "ik_max_word",
  "text": "我是中国人"
}
```

执行结果如下👇

```json
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "CN_CHAR",
      "position" : 0
    },
    {
      "token" : "是",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "CN_CHAR",
      "position" : 1
    },
    {
      "token" : "中国人",
      "start_offset" : 2,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 2
    },
    {
      "token" : "中国",
      "start_offset" : 2,
      "end_offset" : 4,
      "type" : "CN_WORD",
      "position" : 3
    },
    {
      "token" : "国人",
      "start_offset" : 3,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 4
    }
  ]
}
```

:::tip

能够看出不同的分词器，分词有明显的区别，所以以后定义一个索引不能再使用默认的 **`mapping`** 了，要手工建立 mapping，因为要选择分词器

:::

